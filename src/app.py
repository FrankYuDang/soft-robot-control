import time
import logging
from fastapi import Request
import torch
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict

# å¯¼å…¥ä½ çš„æ¨¡å‹å®šä¹‰
# æ³¨æ„ï¼šDocker é‡Œçš„å·¥ä½œç›®å½•æ˜¯ /appï¼Œæ‰€ä»¥ src æ˜¯é¡¶çº§åŒ…
from src.models.lstm_model import AttnLSTM

# 1. å®šä¹‰è¯·æ±‚æ•°æ®çš„æ ¼å¼ (Schema)
# è¿™å°±åƒæ˜¯ API çš„â€œå®‰æ£€é—¨â€ï¼Œä¸ç¬¦åˆæ ¼å¼çš„æ•°æ®ä¼šè¢«ç›´æ¥æŒ¡å›å»
class CableInput(BaseModel):
    # å‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼ŒåŒ…å« 10 ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ï¼Œæ¯ä¸ªæ—¶é—´æ­¥æœ‰ 3 æ ¹çº¿çš„é•¿åº¦
    # ä¾‹å¦‚: [[100, 100, 100], [101, 100, 99], ...]
    sequence: List[List[float]] 

# 1. é…ç½®æ—¥å¿— (Logging Configuration)
# åœ¨å·¥ä¸šç•Œï¼Œæˆ‘ä»¬é€šå¸¸è¾“å‡º JSON æ ¼å¼çš„æ—¥å¿—ï¼Œæ–¹ä¾¿ ELK (Elasticsearch) åˆ†æ
# è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆç”¨æ ‡å‡†æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("soft-robot-api")

app = FastAPI(title="Soft Robot Control API", version="1.2")

# å…¨å±€å˜é‡å­˜æ”¾æ¨¡å‹
model = None
DEVICE = "cpu" # æ¨ç†é€šå¸¸ç”¨ CPU å°±å¤Ÿäº†ï¼Œé™¤éå¹¶å‘é‡æå¤§

# 2. æ’å…¥ä¸­é—´ä»¶ (Middleware) - è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """
    è¿™ä¸ªå‡½æ•°ä¼šæ‹¦æˆªæ¯ä¸€ä¸ªè¯·æ±‚ï¼Œè®°å½•å®ƒè¿›å…¥å’Œç¦»å¼€çš„æ—¶é—´ã€‚
    """
    start_time = time.time()
    
    # å¤„ç†è¯·æ±‚
    response = await call_next(request)
    
    # è®¡ç®—è€—æ—¶ (æ¯«ç§’)
    process_time = (time.time() - start_time) * 1000
    
    # 3. æ‰“å°æ—¥å¿—
    # çœŸæ­£çš„ CTO ä¼šå…³æ³¨ï¼šè¿™ä¸ªè¯·æ±‚èŠ±äº†å¤šä¹…ï¼ŸçŠ¶æ€ç æ˜¯å¤šå°‘ï¼Ÿ
    logger.info(f"Path: {request.url.path} | Method: {request.method} | Status: {response.status_code} | Latency: {process_time:.2f}ms")
    
    # æŠŠè€—æ—¶ä¹ŸåŠ åˆ° Response Header é‡Œï¼Œæ–¹ä¾¿å®¢æˆ·ç«¯æŸ¥çœ‹
    response.headers["X-Process-Time"] = str(process_time)
    
    return response

# 2. å¯åŠ¨äº‹ä»¶ï¼šAPI å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡
@app.on_event("startup")
def load_model():
    global model
    print("ğŸ¤– Loading Soft Robot Brain...")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹æ¶æ„ (å‚æ•°å¿…é¡»å’Œä½ è®­ç»ƒæ—¶çš„ä¸€è‡´ï¼)
        # å¦‚æœä½ è®­ç»ƒæ—¶ç”¨äº† hidden_dim=32, è¿™é‡Œä¹Ÿå¾—æ˜¯ 32
        model = AttnLSTM(
            input_dim=3, 
            hidden_dim=256, 
            output_dim=3, 
            num_heads=4
        )
        
        # åŠ è½½æƒé‡
        # æ³¨æ„è·¯å¾„ï¼šåœ¨ Docker é‡Œï¼Œæˆ‘ä»¬æŒ‚è½½çš„ç›®å½•æ˜¯ /app/data
        model_path = "/app/data/trained_model.pth"
        
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path, map_location=DEVICE))
            model.to(DEVICE)
            model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropout ç­‰)
            print(f"âœ… Model loaded successfully from {model_path}")
        else:
            print(f"âš ï¸ Warning: Model file not found at {model_path}. API will run but predictions will fail.")
            
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")

@app.get("/")
def health_check():
    return {"status": "active", "model_loaded": model is not None}

# 3. é¢„æµ‹æ¥å£
@app.post("/predict")
def predict_coordinates(input_data: CableInput):
    """
    æ¥æ”¶æ‹‰çº¿é•¿åº¦åºåˆ—ï¼Œè¿”å›é¢„æµ‹çš„æœ«ç«¯åæ ‡ (x, y, z)
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # A. æ•°æ®é¢„å¤„ç†
        # âš ï¸ CRITICAL TODO: è¿™é‡Œå…¶å®éœ€è¦åŠ ä¸Šå½’ä¸€åŒ– (Scaler) é€»è¾‘
        # ä½ çš„æ¨¡å‹æ˜¯ç”¨å½’ä¸€åŒ–æ•°æ®(0-1)è®­ç»ƒçš„ï¼Œå¦‚æœä¼ å…¥çœŸå®é•¿åº¦(100mm)ï¼Œé¢„æµ‹ä¼šä¸å‡†ã€‚
        # ä¸ºäº†æ¼”ç¤ºæµç¨‹ï¼Œæˆ‘ä»¬å…ˆå‡è®¾ä¼ å…¥çš„æ•°æ®å·²ç»æ˜¯å½’ä¸€åŒ–è¿‡çš„ã€‚
        
        # å°† list è½¬ä¸º tensor: [1, seq_len, input_dim]
        input_tensor = torch.tensor(input_data.sequence, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        
        # B. æ¨¡å‹æ¨ç†
        with torch.no_grad():
            output_tensor = model(input_tensor) # output: [1, 3]
            
        # C. ç»“æœåå¤„ç†
        # åŒæ ·ï¼Œè¿™é‡Œåº”è¯¥åå½’ä¸€åŒ– (Inverse Transform) æ‰èƒ½å¾—åˆ°æ¯«ç±³å€¼
        prediction = output_tensor.cpu().numpy().tolist()[0]
        
        return {
            "predicted_coordinates": {
                "x": prediction[0],
                "y": prediction[1],
                "z": prediction[2]
            },
            "raw_output": prediction
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")