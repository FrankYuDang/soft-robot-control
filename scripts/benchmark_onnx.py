import time
import numpy as np
import torch
import onnxruntime as ort
import sys
import os

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.models.lstm_model import AttnLSTM

def benchmark():
    # 1. å‡†å¤‡æ•°æ®
    # Batch Size = 1 (æ¨¡æ‹Ÿå®æ—¶å•æ¬¡è¯·æ±‚), Seq Len = 10, Input Dim = 3
    dummy_input = torch.randn(1, 10, 3, dtype=torch.float32)
    numpy_input = dummy_input.numpy() # ONNX éœ€è¦ numpy æ ¼å¼

    print("ğŸ”¥ Warming up models...")
    
# --- Load PyTorch ---
    # å¿…é¡»ä¸¥æ ¼åŒ¹é…è®­ç»ƒæ—¶çš„å‚æ•°ï¼
    pt_model = AttnLSTM(
        input_dim=3, 
        hidden_dim=256, 
        num_layers=2,   # ä½ çš„æ¨¡å‹åªæœ‰ 2 å±‚ LSTM
        output_dim=3,   # è¾“å‡º x, y, z å…± 3 ä¸ªå€¼
        num_heads=4
    )
    pt_model.load_state_dict(torch.load("./data/trained_model.pth", map_location="cpu"))
    pt_model.eval()
    
    # --- Load ONNX ---
    # åˆ›å»ºæ¨ç†ä¼šè¯ (Session)
    ort_session = ort.InferenceSession("./data/soft_robot_model.onnx")
    
    # é¢„çƒ­ (Warmup) - è®© CPU ç¼“å­˜åŠ è½½å¥½
    for _ in range(10):
        pt_model(dummy_input)
        ort_session.run(None, {"input": numpy_input})

    print("ğŸš€ Starting Benchmark (1000 iterations)...")

    # --- Test PyTorch ---
    start = time.time()
    for _ in range(1000):
        with torch.no_grad():
            pt_model(dummy_input)
    pt_time = (time.time() - start) * 1000 / 1000 # å¹³å‡è€—æ—¶ (ms)

    # --- Test ONNX ---
    start = time.time()
    for _ in range(1000):
        # run(output_names, input_feed)
        ort_session.run(None, {"input": numpy_input})
    onnx_time = (time.time() - start) * 1000 / 1000 # å¹³å‡è€—æ—¶ (ms)

    # --- Report ---
    print("\n" + "="*30)
    print(f"ğŸ¢ PyTorch Latency: {pt_time:.4f} ms")
    print(f"âš¡ ONNX Latency:    {onnx_time:.4f} ms")
    print(f"ğŸš€ Speedup:         {pt_time / onnx_time:.2f}x")
    print("="*30)

if __name__ == "__main__":
    benchmark()